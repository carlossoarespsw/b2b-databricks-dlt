# Databricks notebook source
import dlt
import yaml
from pyspark.sql.functions import current_timestamp

# COMMAND ----------
# Caminho correto no DLT (Repo)
config_path = "/Repos/sp_b2b_ops_bot/b2b-databricks-dlt-dev/config/tables_vcn_b2b.yaml"

with open(config_path, "r") as f:
    config = yaml.safe_load(f)

SOURCE_CATALOG = config["source_catalog"]
TABLES = config.get("tables", [])

ENVIRONMENT = spark.conf.get("pipeline.env", "dev")
SOURCE_SYSTEM = "VCN_B2B"

# COMMAND ----------
def create_table(table_conf):
    schema_name = table_conf["schema"]
    table_name = table_conf["name"]

    source_fqn = f"{SOURCE_CATALOG}.{schema_name}.{table_name}"

    @dlt.table(
        name=table_name,
        comment=f"Full load from {source_fqn}",
        table_properties={
            "quality": "bronze",
            "source_system": SOURCE_SYSTEM,
            "environment": ENVIRONMENT,
        },
    )
    def load_table(source_fqn=source_fqn):  # fixa valor por tabela
        return (
            spark.read.table(source_fqn)
            .withColumn("_ingestion_ts", current_timestamp())
        )

# COMMAND ----------
for t in TABLES:
    create_table(t)